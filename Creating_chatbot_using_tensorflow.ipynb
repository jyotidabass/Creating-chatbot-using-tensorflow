{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtnv5KI5EmyDgmPbPHRxMr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyotidabass/Creating-chatbot-using-tensorflow/blob/main/Creating_chatbot_using_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Dummy data for finance application\n",
        "intents = [\n",
        "    {\"tag\": \"greeting\", \"patterns\": [\"Hi\", \"Hey\", \"Hello\"], \"responses\": [\"Hi, how can I assist you?\", \"Hello, what can I help you with?\"]},\n",
        "    {\"tag\": \"stock_price\", \"patterns\": [\"What is the stock price of Apple?\", \"Apple stock price\"], \"responses\": [\"The current stock price of Apple is $100\", \"The stock price of Apple is $100\"]},\n",
        "    {\"tag\": \"portfolio\", \"patterns\": [\"What is my portfolio?\", \"My portfolio\"], \"responses\": [\"Your portfolio consists of Apple, Google, and Amazon stocks\", \"You have invested in Apple, Google, and Amazon stocks\"]},\n",
        "]\n",
        "\n",
        "# Preprocessing data\n",
        "tags = [intent[\"tag\"] for intent in intents]\n",
        "patterns = []\n",
        "responses = []\n",
        "\n",
        "for intent in intents:\n",
        "    patterns.extend(intent[\"patterns\"])\n",
        "    responses.extend(intent[\"responses\"])\n",
        "\n",
        "# Convert data to numerical values\n",
        "patterns = np.array(patterns)\n",
        "responses = np.array(responses)\n",
        "\n",
        "# Create a dictionary to map tags to numerical values\n",
        "tag_values = {tag: i for i, tag in enumerate(tags)}\n",
        "\n",
        "# Restructure tags to match patterns\n",
        "tags_for_patterns = []\n",
        "for intent in intents:\n",
        "    for _ in intent[\"patterns\"]:\n",
        "        tags_for_patterns.append(tag_values[intent[\"tag\"]])\n",
        "\n",
        "tags = np.array(tags_for_patterns)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_patterns, test_patterns, train_tags, test_tags = train_test_split(patterns, tags, test_size=0.2, random_state=42)\n",
        "\n",
        "# One-hot encoding for tags\n",
        "# Fixed: Specify num_classes for to_categorical\n",
        "train_tags = to_categorical(train_tags, num_classes=len(tag_values))\n",
        "test_tags = to_categorical(test_tags, num_classes=len(tag_values))\n",
        "\n",
        "# Create a TextVectorization layer\n",
        "max_tokens = 1000\n",
        "vectorizer = TextVectorization(max_tokens=max_tokens, output_sequence_length=10)\n",
        "\n",
        "# Adapt the vectorizer to your training data\n",
        "vectorizer.adapt(train_patterns)\n",
        "\n",
        "# Vectorize your training and testing patterns\n",
        "train_patterns_vec = vectorizer(train_patterns)\n",
        "test_patterns_vec = vectorizer(test_patterns)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation=\"relu\", input_shape=(train_patterns_vec.shape[1],)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(tag_values), activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_patterns_vec, train_tags, epochs=50, batch_size=32, validation_data=(test_patterns_vec, test_tags))\n",
        "\n",
        "# Define a function to predict the tag\n",
        "def predict_tag(pattern):\n",
        "    pattern_vec = vectorizer(np.array([pattern]))\n",
        "    prediction = model.predict(pattern_vec)\n",
        "    return np.argmax(prediction)\n",
        "\n",
        "# Define a function to get the response\n",
        "def get_response(tag, patterns):\n",
        "    for intent in intents:\n",
        "        if intent[\"tag\"] == list(tag_values.keys())[list(tag_values.values()).index(tag)]:\n",
        "            return np.random.choice(intent[\"responses\"])\n",
        "    return \"I didn't understand that.\"\n",
        "\n",
        "# Test the chatbot\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() == \"quit\":\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    tag = predict_tag(user_input)\n",
        "    response = get_response(tag, patterns)\n",
        "    print(\"Chatbot: \", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IGDwckvU04X",
        "outputId": "160e4e31-4f0b-466b-89a7-43e04a0b9801"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.4000 - loss: 1.5539 - val_accuracy: 1.0000 - val_loss: 1.0090\n",
            "Epoch 2/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4000 - loss: 1.2907 - val_accuracy: 1.0000 - val_loss: 1.0128\n",
            "Epoch 3/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4000 - loss: 1.1167 - val_accuracy: 1.0000 - val_loss: 1.0151\n",
            "Epoch 4/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6000 - loss: 1.2382 - val_accuracy: 1.0000 - val_loss: 1.0169\n",
            "Epoch 5/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2000 - loss: 1.7640 - val_accuracy: 1.0000 - val_loss: 1.0183\n",
            "Epoch 6/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4000 - loss: 1.1105 - val_accuracy: 1.0000 - val_loss: 1.0208\n",
            "Epoch 7/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6000 - loss: 0.6849 - val_accuracy: 1.0000 - val_loss: 1.0233\n",
            "Epoch 8/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0000e+00 - loss: 1.4823 - val_accuracy: 1.0000 - val_loss: 1.0248\n",
            "Epoch 9/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4000 - loss: 1.0625 - val_accuracy: 1.0000 - val_loss: 1.0255\n",
            "Epoch 10/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6000 - loss: 0.8482 - val_accuracy: 1.0000 - val_loss: 1.0255\n",
            "Epoch 11/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8000 - loss: 0.6367 - val_accuracy: 1.0000 - val_loss: 1.0251\n",
            "Epoch 12/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.4000 - loss: 0.8618 - val_accuracy: 1.0000 - val_loss: 1.0247\n",
            "Epoch 13/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.4000 - loss: 0.8022 - val_accuracy: 1.0000 - val_loss: 1.0247\n",
            "Epoch 14/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8000 - loss: 0.5545 - val_accuracy: 1.0000 - val_loss: 1.0242\n",
            "Epoch 15/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6000 - loss: 0.8549 - val_accuracy: 1.0000 - val_loss: 1.0240\n",
            "Epoch 16/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6000 - loss: 0.9391 - val_accuracy: 1.0000 - val_loss: 1.0229\n",
            "Epoch 17/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8000 - loss: 0.5488 - val_accuracy: 1.0000 - val_loss: 1.0212\n",
            "Epoch 18/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4000 - loss: 0.8366 - val_accuracy: 1.0000 - val_loss: 1.0187\n",
            "Epoch 19/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6000 - loss: 0.8198 - val_accuracy: 1.0000 - val_loss: 1.0167\n",
            "Epoch 20/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8000 - loss: 0.5159 - val_accuracy: 1.0000 - val_loss: 1.0153\n",
            "Epoch 21/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6000 - loss: 0.8857 - val_accuracy: 1.0000 - val_loss: 1.0134\n",
            "Epoch 22/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6000 - loss: 0.6869 - val_accuracy: 1.0000 - val_loss: 1.0120\n",
            "Epoch 23/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.4000 - loss: 0.7261 - val_accuracy: 1.0000 - val_loss: 1.0111\n",
            "Epoch 24/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8000 - loss: 0.5879 - val_accuracy: 1.0000 - val_loss: 1.0097\n",
            "Epoch 25/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8000 - loss: 0.4789 - val_accuracy: 1.0000 - val_loss: 1.0085\n",
            "Epoch 26/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6000 - loss: 0.5911 - val_accuracy: 1.0000 - val_loss: 1.0059\n",
            "Epoch 27/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8000 - loss: 0.5975 - val_accuracy: 1.0000 - val_loss: 1.0040\n",
            "Epoch 28/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.4951 - val_accuracy: 1.0000 - val_loss: 1.0023\n",
            "Epoch 29/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.6000 - loss: 0.5946 - val_accuracy: 1.0000 - val_loss: 1.0005\n",
            "Epoch 30/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6000 - loss: 0.8040 - val_accuracy: 1.0000 - val_loss: 0.9979\n",
            "Epoch 31/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.2517 - val_accuracy: 1.0000 - val_loss: 0.9954\n",
            "Epoch 32/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6000 - loss: 0.6058 - val_accuracy: 1.0000 - val_loss: 0.9928\n",
            "Epoch 33/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8000 - loss: 0.5959 - val_accuracy: 1.0000 - val_loss: 0.9906\n",
            "Epoch 34/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8000 - loss: 0.6190 - val_accuracy: 1.0000 - val_loss: 0.9891\n",
            "Epoch 35/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8000 - loss: 0.5292 - val_accuracy: 1.0000 - val_loss: 0.9880\n",
            "Epoch 36/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8000 - loss: 0.4345 - val_accuracy: 1.0000 - val_loss: 0.9871\n",
            "Epoch 37/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6000 - loss: 0.6124 - val_accuracy: 1.0000 - val_loss: 0.9868\n",
            "Epoch 38/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8000 - loss: 0.3626 - val_accuracy: 1.0000 - val_loss: 0.9863\n",
            "Epoch 39/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8000 - loss: 0.5235 - val_accuracy: 1.0000 - val_loss: 0.9851\n",
            "Epoch 40/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.4306 - val_accuracy: 1.0000 - val_loss: 0.9839\n",
            "Epoch 41/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8000 - loss: 0.3511 - val_accuracy: 1.0000 - val_loss: 0.9828\n",
            "Epoch 42/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.4093 - val_accuracy: 1.0000 - val_loss: 0.9821\n",
            "Epoch 43/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8000 - loss: 0.5139 - val_accuracy: 1.0000 - val_loss: 0.9797\n",
            "Epoch 44/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.3145 - val_accuracy: 1.0000 - val_loss: 0.9776\n",
            "Epoch 45/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.3210 - val_accuracy: 1.0000 - val_loss: 0.9761\n",
            "Epoch 46/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.4329 - val_accuracy: 1.0000 - val_loss: 0.9737\n",
            "Epoch 47/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.3554 - val_accuracy: 1.0000 - val_loss: 0.9719\n",
            "Epoch 48/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8000 - loss: 0.3280 - val_accuracy: 1.0000 - val_loss: 0.9703\n",
            "Epoch 49/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.1738 - val_accuracy: 1.0000 - val_loss: 0.9687\n",
            "Epoch 50/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.2792 - val_accuracy: 1.0000 - val_loss: 0.9666\n",
            "User: hi\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "Chatbot:  Hello, what can I help you with?\n",
            "User: What is the stock price of Apple?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Chatbot:  The current stock price of Apple is $100\n",
            "User: What is my portfolio?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Chatbot:  Your portfolio consists of Apple, Google, and Amazon stocks\n",
            "User: quit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    }
  ]
}